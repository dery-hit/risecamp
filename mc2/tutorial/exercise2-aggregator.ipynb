{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiparty XGBoost with Centralized Training\n",
    "In this exercise, we'll demonstrate a workflow in which each party has its own data and sends a copy of its data to the central server. Therefore, all the training data is sent over the network to the central server, who collects it and locally trains a model on all the data. The central server will then broadcast the trained model back to the parties, who will load the model and test it on their local test datasets. \n",
    "\n",
    "![title](img/exercise2.png)\n",
    "\n",
    "\n",
    "We will also measure the number of bytes sent over the network to show the large bandwidth needed for this workflow. \n",
    "This shows the benefits of using as much data as possible to make the model more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transfer\n",
    "Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from Utils import scp, PKI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though we don't need to do this part, we think it's helpful to see how many bytes would be transferred over the network if you weren't the aggregator and had to send your training data over the network. Send the training data you used in Exercise 1 over the network to your `~/shared_data` directory. Note how many bytes are transferred.\n",
    "\n",
    "* Training data for the insurance dataset is at `/data/insurance/insurance_training_{party_id}.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the PKI to help with IP lookups\n",
    "pki = PKI()\n",
    "aggregator = \"\" # TODO: fill in your username here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you use the training data you used in exercise 1\n",
    "training_data = \"/path/to/training_data\" # TODO: fill in the path to the training data\n",
    "my_ip = pki.lookup(aggregator)[0] # Get your IP\n",
    "dest_dir = \"~/shared_data\"\n",
    "scp(training_data, my_ip, dest_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data will be sent from all parties to your `~/shared_data/` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate the Received Data\n",
    "Wait for all parties to send you their data and load all the data that has been sent to your machine. For example, if three other parties sent you data, make 4 calls to `read_csv()`: one for your own data and three for the other parties' data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate all the data in preparation for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_lst = []\n",
    "\n",
    "# TODO: add the paths to all shared data to shared_data_path_lst\n",
    "shared_data_path_lst = []\n",
    "\n",
    "for path in shared_data_path_lst:\n",
    "    training_data_subset = pd.read_csv(path, sep=\",\", header=None)\n",
    "    training_data_lst.append(training_data_subset)\n",
    "\n",
    "aggregated_training_data = pd.concat(training_data_lst) \n",
    "aggregated_training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the aggregated training data into features and labels\n",
    "y_agg_train = aggregated_training_data.iloc[:, 0]\n",
    "x_agg_train = aggregated_training_data.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg1, arg2 = # TODO: fill these variables in with the aggregated features and labels\n",
    "\n",
    "multiparty_model = xgb.XGBClassifier()\n",
    "multiparty_model.fit(arg1, arg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcast the Trained Model\n",
    "Save the trained model and send it to all parties in the federation. The model will be sent to the home directory of each party."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "model_name = \"ex2_model.model\"\n",
    "pickle.dump(multiparty_model, open(model_name, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're the central server, run this cell as many times as needed to send the saved model\n",
    "# to all parties in the federation\n",
    "model_file = \"ex2_model.model\"\n",
    "dest_dir = \"~\"\n",
    "dest_ips = []\n",
    "\n",
    "# TODO: fill in the usernames of all members of your federation\n",
    "# No need to include your own username here\n",
    "members = []\n",
    "\n",
    "for member in members:\n",
    "    member_ip = pki.lookup(member)[0]\n",
    "    dest_ips.append(member_ip)\n",
    "\n",
    "for ip in dest_ips:\n",
    "    scp(model_file, ip, dest_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "Load in your local test data and preprocess it to split it into features and labels. Then evaluate your model.\n",
    "* Test data for the insurance dataset is at `/data/insurance/insurance_test_{party_id}.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in your local test data and preprocess it to split it into features and labels\n",
    "test_data_path = \"path/to/test/data\" # TODO: fill in the path to your test data\n",
    "test_data = pd.read_csv(test_data_path, sep=\",\", header=None)\n",
    "y_test = test_data.iloc[:, 0]\n",
    "x_test = test_data.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg1, arg2 = # TODO: set arg1 to the test features, arg2 to the test labels\n",
    "preds = multiparty_model.predict(arg1)\n",
    "print(accuracy_score(arg2, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss the results with other members of your federation. How did the centrally trained model perform on your local test data compared with the locally trained model? Did adding more data help?\n",
    "\n",
    "Once you're ready, please move to [Exercise 3](./exercise3-aggregator.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
